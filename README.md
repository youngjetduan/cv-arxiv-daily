[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]

## Updated on 2025.02.03
> Usage instructions: [here](./docs/README.md#usage)

> Other links:
<details>
  <summary>Table of Contents</summary>
  <ol>
    <li><a href=#llm>LLM</a></li>
    <li><a href=#early-stopping>Early Stopping</a></li>
  </ol>
</details>

## LLM

|ID|Publish Date|Title|Authors|PDF|Code|Kimi|
|---|---|---|---|---|---|---|
| 1|**2025-01-31**|**Scalable-Softmax Is Superior for Attention**|Ken M. Nakanishi et.al.|[2501.19399](http://arxiv.org/pdf/2501.19399)|null|[Kimi](https://papers.cool/arxiv/2501.19399)|
| 2|**2025-01-31**|**Cache Me If You Must: Adaptive Key-Value Quantization for Large Language Models**|Alina Shutova et.al.|[2501.19392](http://arxiv.org/pdf/2501.19392)|null|[Kimi](https://papers.cool/arxiv/2501.19392)|
| 3|**2025-01-31**|**Efficient Reasoning with Hidden Thinking**|Xuan Shen et.al.|[2501.19201](http://arxiv.org/pdf/2501.19201)|**[link](https://github.com/shawnricecake/heima)**|[Kimi](https://papers.cool/arxiv/2501.19201)|
| 4|**2025-01-31**|**Rethinking Early Stopping: Refine, Then Calibrate**|Eug√®ne Berta et.al.|[2501.19195](http://arxiv.org/pdf/2501.19195)|**[link](https://github.com/eugeneberta/refinethencalibrate-theory)**|[Kimi](https://papers.cool/arxiv/2501.19195)|
| 5|**2025-01-31**|**A theoretical framework for overfitting in energy-based modeling**|Giovanni Catania et.al.|[2501.19158](http://arxiv.org/pdf/2501.19158)|null|[Kimi](https://papers.cool/arxiv/2501.19158)|
| 6|**2025-01-31**|**$\infty$ -Video: A Training-Free Approach to Long Video Understanding via Continuous-Time Memory Consolidation**|Saul Santos et.al.|[2501.19098](http://arxiv.org/pdf/2501.19098)|**[link](https://github.com/deep-spin/infinite-video)**|[Kimi](https://papers.cool/arxiv/2501.19098)|
| 7|**2025-01-30**|**Rope to Nope and Back Again: A New Hybrid Attention Strategy**|Bowen Yang et.al.|[2501.18795](http://arxiv.org/pdf/2501.18795)|null|[Kimi](https://papers.cool/arxiv/2501.18795)|
| 8|**2025-01-30**|**Zero-shot Large Language Models for Long Clinical Text Summarization with Temporal Reasoning**|Maya Kruse et.al.|[2501.18724](http://arxiv.org/pdf/2501.18724)|null|[Kimi](https://papers.cool/arxiv/2501.18724)|
| 9|**2025-01-30**|**Rethinking Bottlenecks in Safety Fine-Tuning of Vision Language Models**|Yi Ding et.al.|[2501.18533](http://arxiv.org/pdf/2501.18533)|null|[Kimi](https://papers.cool/arxiv/2501.18533)|
|10|**2025-01-30**|**State Stream Transformer (SST) : Emergent Metacognitive Behaviours Through Latent State Persistence**|Thea Aviss et.al.|[2501.18356](http://arxiv.org/pdf/2501.18356)|null|[Kimi](https://papers.cool/arxiv/2501.18356)|

<p align=right>(<a href=#updated-on-20250203>back to top</a>)</p>

## Early Stopping

|ID|Publish Date|Title|Authors|PDF|Code|Kimi|
|---|---|---|---|---|---|---|
| 1|**2024-12-12**|**InstanceCap: Improving Text-to-Video Generation via Instance-aware Structured Caption**|Tiehan Fan et.al.|[2412.09283](http://arxiv.org/abs/2412.09283)|null|[Kimi](https://papers.cool/arxiv/2412.09283)|
| 2|**2024-12-11**|**GradStop: Exploring Training Dynamics in Unsupervised Outlier Detection through Gradient Cohesion**|Yuang Zhang et.al.|[2412.08501](http://arxiv.org/abs/2412.08501)|**[link](https://github.com/Yann-zh/gradAE)**|[Kimi](https://papers.cool/arxiv/2412.08501)|
| 3|**2024-12-11**|**Collaborative Inference for Large Models with Task Offloading and Early Exiting**|Zuan Xie et.al.|[2412.08284](http://arxiv.org/abs/2412.08284)|null|[Kimi](https://papers.cool/arxiv/2412.08284)|
| 4|**2024-12-11**|**Diff-GO $^\text{n}$ : Enhancing Diffusion Models for Goal-Oriented Communications**|Suchinthaka Wanninayaka et.al.|[2412.06980](http://arxiv.org/abs/2412.06980)|null|[Kimi](https://papers.cool/arxiv/2412.06980)|
| 5|**2024-12-06**|**Sparse autoencoders reveal selective remapping of visual concepts during adaptation**|Hyesu Lim et.al.|[2412.05276](http://arxiv.org/abs/2412.05276)|**[link](https://github.com/dynamical-inference/patchsae)**|[Kimi](https://papers.cool/arxiv/2412.05276)|
| 6|**2024-12-06**|**BEExformer: A Fast Inferencing Transformer Architecture via Binarization with Multiple Early Exits**|Wazib Ansar et.al.|[2412.05225](http://arxiv.org/abs/2412.05225)|null|[Kimi](https://papers.cool/arxiv/2412.05225)|
| 7|**2024-12-05**|**A Stitch in Time Saves Nine: Small VLM is a Precise Guidance for Accelerating Large VLMs**|Wangbo Zhao et.al.|[2412.03324](http://arxiv.org/abs/2412.03324)|**[link](https://github.com/NUS-HPC-AI-Lab/SGL)**|[Kimi](https://papers.cool/arxiv/2412.03324)|
| 8|**2024-12-03**|**Time-Series-Informed Closed-loop Learning for Sequential Decision Making and Control**|Sebastian Hirt et.al.|[2412.02423](http://arxiv.org/abs/2412.02423)|null|[Kimi](https://papers.cool/arxiv/2412.02423)|
| 9|**2024-12-02**|**Early Exit Is a Natural Capability in Transformer-based Models: An Empirical Study on Early Exit without Joint Optimization**|Weiqiao Shan et.al.|[2412.01455](http://arxiv.org/abs/2412.01455)|null|[Kimi](https://papers.cool/arxiv/2412.01455)|
|10|**2024-12-02**|**EdgeOAR: Real-time Online Action Recognition On Edge Devices**|Wei Luo et.al.|[2412.01267](http://arxiv.org/abs/2412.01267)|null|[Kimi](https://papers.cool/arxiv/2412.01267)|

<p align=right>(<a href=#updated-on-20250203>back to top</a>)</p>

[contributors-shield]: https://img.shields.io/github/contributors/keyunj/cv-arxiv-daily.svg?style=for-the-badge
[contributors-url]: https://github.com/keyunj/cv-arxiv-daily/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/keyunj/cv-arxiv-daily.svg?style=for-the-badge
[forks-url]: https://github.com/keyunj/cv-arxiv-daily/network/members
[stars-shield]: https://img.shields.io/github/stars/keyunj/cv-arxiv-daily.svg?style=for-the-badge
[stars-url]: https://github.com/keyunj/cv-arxiv-daily/stargazers
[issues-shield]: https://img.shields.io/github/issues/keyunj/cv-arxiv-daily.svg?style=for-the-badge
[issues-url]: https://github.com/keyunj/cv-arxiv-daily/issues

